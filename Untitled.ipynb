{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdf322a-44f2-4fb2-9e23-d236a2e9ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c86ce-4b5c-4a9c-b225-cd1d3f21f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Path to the main RBC zip file\n",
    "main_zip_path = \"data/RBCdataset.zip\"\n",
    "extracted_root = \"data/RBC_extracted\"\n",
    "\n",
    "# Step 1: Extract main zip (RBCdataset.zip)\n",
    "os.makedirs(extracted_root, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(main_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_root)\n",
    "    print(\"✅ Extracted main RBCdataset.zip\")\n",
    "\n",
    "# Step 2: Extract each nested zip file (1 Elliptocyte 1211.zip, etc.)\n",
    "rbc_types_dir = os.path.join(extracted_root, \"RBCdataset\")  # this folder is inside the main zip\n",
    "final_dataset_dir = os.path.join(\"data\", \"RBC_final\")\n",
    "os.makedirs(final_dataset_dir, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(rbc_types_dir):\n",
    "    if file_name.lower().endswith(\".zip\"):\n",
    "        file_path = os.path.join(rbc_types_dir, file_name)\n",
    "        folder_name = file_name.replace(\".zip\", \"\").replace(\" \", \"_\")\n",
    "        extract_path = os.path.join(final_dataset_dir, folder_name)\n",
    "        os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        print(f\"✅ Extracted: {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c44f69-3015-483d-81dd-5ebaf53ed63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Correct path based on your screenshot\n",
    "peripheral_zip_path = \"data/PeripheralBloodSmear.zip\"\n",
    "peripheral_extract_root = \"data/Peripheral_extracted\"\n",
    "final_pbs_dir = \"data/PBS_final\"\n",
    "\n",
    "# Make sure output folders exist\n",
    "os.makedirs(peripheral_extract_root, exist_ok=True)\n",
    "os.makedirs(final_pbs_dir, exist_ok=True)\n",
    "\n",
    "# Step 1: Extract the main PeripheralBloodSmear.zip\n",
    "with zipfile.ZipFile(peripheral_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(peripheral_extract_root)\n",
    "    print(\"✅ Extracted PeripheralBloodSmear.zip\")\n",
    "\n",
    "# Step 2: Extract all nested zip files inside Peripheral_extracted\n",
    "for root, dirs, files in os.walk(peripheral_extract_root):\n",
    "    for file_name in files:\n",
    "        if file_name.lower().endswith(\".zip\"):\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            folder_name = file_name.replace(\".zip\", \"\").replace(\" \", \"_\")\n",
    "            extract_path = os.path.join(final_pbs_dir, folder_name)\n",
    "            os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "            print(f\"✅ Extracted: {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f89ea29-feb9-4222-ad50-32c1f1a8b69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from opencv-python) (2.2.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64213879-297c-43f3-a9d3-c080a0357266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pillow in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (11.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccab2876-626c-475e-8517-0189e2e40094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "509f4627-e2fe-424f-a26b-5b7a8731c8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fcab6f-225e-41c9-b4b6-db95c1d9d395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf38455-a546-423c-993c-02e7d63d4ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db715181-864f-49dc-b1eb-10ea587ba29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torchvision) (2.2.4)\n",
      "Requirement already satisfied: torch==2.6.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.6.0->torchvision) (4.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.6.0->torchvision) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964892e6-20a7-4315-ad41-5147ab72ea95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be93e7a2-25e9-45f0-a388-2001bd37b0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: timm in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (1.0.15)\n",
      "Requirement already satisfied: torch in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from timm) (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from timm) (0.21.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from timm) (0.30.2)\n",
      "Requirement already satisfied: safetensors in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub->timm) (4.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torch->timm) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torchvision->timm) (2.2.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from torchvision->timm) (11.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marya\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e158164b-8ff0-49bf-a435-f357a54b56d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marya\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433d4f18-1a73-437e-83ac-9665776323a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle, resample, class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision.models import convnext_tiny\n",
    "\n",
    "# ⚙️ Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 📂 Paths\n",
    "TARGET_SIZE = (224, 224)\n",
    "RBC_ROOT = \"data/RBC_final\"\n",
    "PBS_ROOT = \"data/PBS_final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57747bba-23b4-4e71-9b2f-e152e097ef54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading RBC dataset...\n",
      "📸 Found 7108 images in data/RBC_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RBC_final: 100%|████████████████████████████████████████████████████████| 7108/7108 [46:02<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading PBS dataset...\n",
      "📸 Found 80 images in data/PBS_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PBS_final: 100%|████████████████████████████████████████████████████████████| 80/80 [00:51<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 🔧 Image preprocessing\n",
    "def preprocess_image(img_path, target_size=(224, 224)):\n",
    "    try:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "        if img is None:\n",
    "            raise Exception(\"Image could not be loaded\")\n",
    "        img = cv2.resize(img, target_size)\n",
    "\n",
    "        if len(img.shape) == 2 or (len(img.shape) == 3 and img.shape[2] == 1):\n",
    "            img = cv2.fastNlMeansDenoising(img, None, h=10, templateWindowSize=7, searchWindowSize=21)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        else:\n",
    "            img = cv2.fastNlMeansDenoisingColored(img, None, h=10, hColor=10, templateWindowSize=7, searchWindowSize=21)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# 🧹 Dataset loading\n",
    "def load_dataset_from_folder(root_folder, label):\n",
    "    data, labels, image_paths = [], [], []\n",
    "    for subdir, _, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "    print(f\"📸 Found {len(image_paths)} images in {root_folder}\")\n",
    "    for img_path in tqdm(image_paths, desc=f\"Processing {os.path.basename(root_folder)}\"):\n",
    "        img = preprocess_image(img_path, TARGET_SIZE)\n",
    "\n",
    "        if img is not None:\n",
    "            data.append(img)\n",
    "            labels.append(label)\n",
    "    return data, labels\n",
    "\n",
    "# 🧪 Load both datasets\n",
    "print(\"🔄 Loading RBC dataset...\")\n",
    "rbc_data, rbc_labels = load_dataset_from_folder(RBC_ROOT, label=0)\n",
    "print(\"🔄 Loading PBS dataset...\")\n",
    "pbs_data, pbs_labels = load_dataset_from_folder(PBS_ROOT, label=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a6873-84a7-4ef4-b9c2-be47c0518f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ----------------------\n",
    "# Custom Dataset\n",
    "# ----------------------\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data, labels, augment=False):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx].astype(np.float32) / 255.0\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        if self.augment:\n",
    "            if np.random.rand() > 0.5:\n",
    "                image = np.fliplr(image).copy()\n",
    "            if np.random.rand() > 0.5:\n",
    "                image = np.flipud(image).copy()\n",
    "            if np.random.rand() > 0.5:\n",
    "                angle = np.random.choice([90, 180, 270])\n",
    "                image = cv2.rotate(image, {\n",
    "                    90: cv2.ROTATE_90_CLOCKWISE,\n",
    "                    180: cv2.ROTATE_180,\n",
    "                    270: cv2.ROTATE_90_COUNTERCLOCKWISE\n",
    "                }[angle])\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        image = (image - 0.5) / 0.5\n",
    "        image = torch.tensor(image.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# ----------------------\n",
    "# Step 1: Prepare Data\n",
    "# ----------------------\n",
    "# Assume rbc_data, rbc_labels, pbs_data, pbs_labels already defined\n",
    "from sklearn.utils import resample\n",
    "\n",
    "print(\"🔁 Oversampling PBS to balance...\")\n",
    "pbs_data_upsampled, pbs_labels_upsampled = resample(\n",
    "    np.array(pbs_data), np.array(pbs_labels),\n",
    "    replace=True,\n",
    "    n_samples=len(rbc_data),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Merge and shuffle\n",
    "all_data = np.array(rbc_data + list(pbs_data_upsampled))\n",
    "all_labels = np.array(rbc_labels + list(pbs_labels_upsampled))\n",
    "all_data, all_labels = shuffle(all_data, all_labels, random_state=42)\n",
    "print(f\"✅ Final balanced dataset: {len(all_data)} images\")\n",
    "\n",
    "# ----------------------\n",
    "# Step 2: Split + Balance Train\n",
    "# ----------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    all_data, all_labels,\n",
    "    test_size=0.2,\n",
    "    stratify=all_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rbc_train = X_train[y_train == 0]\n",
    "pbs_train = X_train[y_train == 1]\n",
    "rbc_labels_train = y_train[y_train == 0]\n",
    "pbs_labels_train = y_train[y_train == 1]\n",
    "\n",
    "pbs_upsampled, pbs_labels_upsampled = resample(\n",
    "    pbs_train, pbs_labels_train,\n",
    "    replace=True,\n",
    "    n_samples=len(rbc_train),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_bal = np.concatenate([rbc_train, pbs_upsampled])\n",
    "y_train_bal = np.concatenate([rbc_labels_train, pbs_labels_upsampled])\n",
    "X_train_bal, y_train_bal = shuffle(X_train_bal, y_train_bal, random_state=42)\n",
    "\n",
    "# Check distribution\n",
    "print(\"🔍 Train label distribution:\", Counter(y_train_bal))\n",
    "print(\"🔍 Val label distribution:\", Counter(y_val))\n",
    "\n",
    "# ----------------------\n",
    "# Step 3: Dataloaders\n",
    "# ----------------------\n",
    "train_dataset = CustomImageDataset(X_train_bal, y_train_bal, augment=True)\n",
    "val_dataset = CustomImageDataset(X_val, y_val, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# ----------------------\n",
    "# Step 4: Model Setup\n",
    "# ----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.DEFAULT)\n",
    "\n",
    "# Freeze feature extractor (optional, for stable training)\n",
    "# for param in model.features.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Replace classifier head\n",
    "model.classifier[2] = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(model.classifier[2].in_features, 2)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# ----------------------\n",
    "# Step 5: Loss & Optimizer\n",
    "# ----------------------\n",
    "# Use plain cross-entropy first for stability\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# ----------------------\n",
    "# Step 6: Training Loop\n",
    "# ----------------------\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, correct = 0.0, 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"🔁 Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_acc = correct / len(train_dataset)\n",
    "    print(f\"✅ Train Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0.0, 0\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            all_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_dataset)\n",
    "    val_acc = val_correct / len(val_dataset)\n",
    "    print(f\"🧪 Val Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # LR Scheduler & Early Stop\n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"⛔ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Classification report for train set (optional)\n",
    "    print(\"\\n📊 Detailed Train Report:\")\n",
    "    model.eval()\n",
    "    train_preds, train_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            train_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "    print(classification_report(train_true, train_preds, target_names=[\"RBC\", \"PBS\"]))\n",
    "\n",
    "# ----------------------\n",
    "# Step 7: Final Report\n",
    "# ----------------------\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[\"RBC\", \"PBS\"]))\n",
    "\n",
    "print(\"\\n📉 Confusion Matrix:\")\n",
    "print(confusion_matrix(all_targets, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f05268-fab3-4557-8578-4556aebc658a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ad07a-d202-4d41-8ca5-f3272572342f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
